apache spark apache spark is an open source cluster computing framework originally developed at the university of california berkeleys amplab the spark codebase was later donated to the apache software foundation which has maintained it since spark provides an interface for programming entire clusters with implicit data parallelism and faulttolerance overview apache spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset rdd a readonly multiset of data items distributed over a cluster of machines that is maintained in a faulttolerant way it was developed in response to limitations in the mapreduce cluster computing paradigm which forces a particular linear dataflow structure on distributed programs mapreduce programs read input data from disk map a function across the data reduce the results of the map and store reduction results on disk sparks rdds function as a working set for distributed programs that offers a deliberately restricted form of distributed shared memory the availability of rdds facilitates the implementation of both iterative algorithms that visit their dataset multiple times in a loop and interactiveexploratory data analysis ie the repeated databasestyle querying of data the latency of such applications compared to apache hadoop a popular mapreduce implementation may be reduced by several orders of magnitude among the class of iterative algorithms are the training algorithms for machine learning systems which formed the initial impetus for developing apache spark apache spark requires a cluster manager and a distributed storage system for cluster management spark supports standalone native spark cluster hadoop yarn or apache mesos for distributed storage spark can interface with a wide variety including hadoop distributed file system hdfs mapr file system maprfs cassandra openstack swift amazon s kudu or a custom solution can be implemented spark also supports a pseudodistributed local mode usually used only for development or testing purposes where distributed storage is not required and the local file system can be used instead in such a scenario spark is run on a single machine with one executor per cpu core spark core spark core is the foundation of the overall project it provides distributed task dispatching scheduling and basic io functionalities exposed through an application programming interface for java python scala and r centered on the rdd abstraction the java api available is available for other jvm languages but is also useable for some other nonjvm languages such as julia that can connect to the jvm this interface mirrors a functionalhigherorder model of programming a driver program invokes parallel operations such as map filter or reduce on an rdd by passing a function to spark which then schedules the functions execution in parallel on the cluster these operations and additional ones such as joins take rdds as input and produce new rdds rdds are immutable and their operations are lazy faulttolerance is achieved by keeping track of the lineage of each rdd the sequence of operations that produced it so that it can be reconstructed in the case of data loss rdds can contain any type of python java or scala objects aside from the rddoriented functional style of programming spark provides two restricted forms of shared variables broadcast variables reference readonly data that needs to be available on all nodes while accumulators can be used to program reductions in an imperative style a typical example of rddcentric functional programming is the following scala program that computes the frequencies of all words occurring in a set of text files and prints the most common ones each  a variant of  and takes an anonymous function that performs a simple operation on a single data item or a pair of items and applies its argument to transform an rdd into a new rdd spark sql spark sql is a component on top of spark core that introduced a data abstraction called dataframes which provides support for structured and semistructured data spark sql provides a domainspecific language dsl to manipulate dataframes in scala java or python it also provides sql language support with commandline interfaces and odbcjdbc server spark streaming spark streaming leverages spark cores fast scheduling capability to perform streaming analytics it ingests data in minibatches and performs rdd transformations on those minibatches of data this design enables the same set of application code written for batch analytics to be used in streaming analytics thus facilitating easy implementation of lambda architecture however this convenience comes with the penalty of latency equal to the minibatch duration other streaming data engines that process event by event rather than in minibatches include storm and the streaming component of flink spark streaming has support builtin to consume from kafka flume twitter zeromq kinesis and tcpip sockets mllib machine learning library spark mllib is a distributed machine learning framework on top of spark core that due in large part to the distributed memorybased spark architecture is as much as nine times as fast as the diskbased implementation used by apache mahout according to benchmarks done by the mllib developers against the alternating least squares als implementations and before mahout itself gained a spark interface and scales better than vowpal wabbit many common machine learning and statistical algorithms have been implemented and are shipped with mllib which simplifies large scale machine learning pipelines including graphx graphx is a distributed graph processing framework on top of apache spark because it is based on rdds which are immutable graphs are immutable and thus graphx is unsuitable for graphs that need to be updated let alone in a transactional manner like a graph database graphx provides two separate apis for implementation of massively parallel algorithms such as pagerank a pregel abstraction and a more general mapreduce style api unlike its predecessor bagel which was formally deprecated in spark  graphx has full support for property graphs graphs where properties can be attached to edges and vertices graphx can be viewed as being the spark inmemory version of apache giraph which utilized hadoop diskbased mapreduce like apache spark graphx initially started as a research project at uc berkeleys amplab and databricks and was later donated to the apache software foundation and the spark project history spark was initially started by matei zaharia at uc berkeleys amplab in  and open sourced in  under a bsd license in  the project was donated to the apache software foundation and switched its license to apache  in february  spark became a toplevel apache project in november  spark founder m zaharias company databricks set a new world record in large scale sorting using spark spark had in excess of  contributors in  making it one of the most active projects in the apache software foundation and one of the most active open source big data projects