openmp openmp open multiprocessing is an application programming interface api that supports multiplatform shared memory multiprocessing programming in c c and fortran on most platforms processor architectures and operating systems including solaris aix hpux linux os x and windows it consists of a set of compiler directives library routines and environment variables that influence runtime behavior openmp is managed by the nonprofit technology consortium openmp architecture review board or openmp arb jointly defined by a group of major computer hardware and software vendors including amd ibm intel cray hp fujitsu nvidia nec red hat texas instruments oracle corporation and more openmp uses a portable scalable model that gives programmers a simple and flexible interface for developing parallel applications for platforms ranging from the standard desktop computer to the supercomputer an application built with the hybrid model of parallel programming can run on a computer cluster using both openmp and message passing interface mpi such that openmp is used for parallelism within a multicore node while mpi is used for parallelism between nodes there have also been efforts to run openmp on software distributed shared memory systems to translate openmp into mpi and to extend openmp for nonshared memory systems introduction openmp is an implementation of multithreading a method of parallelizing whereby a master thread a series of instructions executed consecutively forks a specified number of slave threads and the system divides a task among them the threads then run concurrently with the runtime environment allocating threads to different processors the section of code that is meant to run in parallel is marked accordingly with a compiler directive that will cause the threads to form before the section is executed each thread has an id attached to it which can be obtained using a function called codice the thread id is an integer and the master thread has an id of  after the execution of the parallelized code the threads join back into the master thread which continues onward to the end of the program by default each thread executes the parallelized section of code independently worksharing constructs can be used to divide a task among the threads so that each thread executes its allocated part of the code both task parallelism and data parallelism can be achieved using openmp in this way the runtime environment allocates threads to processors depending on usage machine load and other factors the runtime environment can assign the number of threads based on environment variables or the code can do so using functions the openmp functions are included in a header file labelled omph in cc history the openmp architecture review board arb published its first api specifications openmp for fortran  in october  october the following year they released the cc standard  saw version  of the fortran specifications with version  of the cc specifications being released in  version  is a combined ccfortran specification that was released in  up to version  openmp primarily specified ways to parallelize highly regular loops as they occur in matrixoriented numerical programming where the number of iterations of the loop is known at entry time this was recognized as a limitation and various task parallel extensions were added to implementations in  an effort to standardize task parallelism was formed which published a proposal in  taking inspiration from task parallelism features in cilk x and chapel version  was released in may  included in the new features in  is the concept of tasks and the task construct significantly broadening the scope of openmp beyond the parallel loop constructs that made up most of openmp  version  of the specification was released in july  it adds or improves the following features support for accelerators atomics error handling thread affinity tasking extensions user defined reduction simd support fortran  support the core elements the core elements of openmp are the constructs for thread creation workload distribution work sharing dataenvironment management thread synchronization userlevel runtime routines and environment variables in cc openmp uses pragmas the openmp specific pragmas are listed below thread creation the pragma omp parallel is used to fork additional threads to carry out the work enclosed in the construct in parallel the original thread will be denoted as master thread with thread id  example c program display hello world using multiple threads int mainvoid use flag fopenmp to compile using gcc  gcc fopenmp helloc o hello output on a computer with two cores and thus two threads hello world hello world however the output may also be garbled because of the race condition caused from the two threads sharing the standard output hello whello woorld rld worksharing constructs used to specify how to assign independent work to one or all of the threads example initialize the value of a large array in parallel using each thread to do part of the work the loop counter is declared inside the parallel loop in c style which gives each thread a unique and private version of the variable openmp clauses since openmp is a shared memory programming model most variables in openmp code are visible to all threads by default but sometimes private variables are necessary to avoid race conditions and there is a need to pass values between the sequential part and the parallel region the code block executed in parallel so data environment management is introduced as data sharing attribute clauses by appending them to the openmp directive the different types of clauses are userlevel runtime routines used to modifycheck the number of threads detect if the execution context is in a parallel region how many processors in current system setunset locks timing functions etc environment variables a method to alter the execution features of openmp applications used to control loop iterations scheduling default number of threads etc for example ompnumthreads is used to specify number of threads for an application sample programs in this section some sample programs are provided to illustrate the concepts explained above hello world a basic program that exercises the parallel private and barrier directives and the functions codice and codice not to be confused c this c program can be compiled using gcc with the flag fopenmp int main int argc char argv  c this c program can be compiled using gcc g wall wextra werror fopenmp testcpp note iostream is not threadsafe therefore for instance cout calls must be executed in critical areas or by only one thread eg masterthread int main fortran  here is a fortran  version comp parallel privateid comp barrier comp end parallel fortran  freeform here is a fortran  freeform version clauses in worksharing constructs in cc the application of some openmp clauses are illustrated in the simple examples in this section the code sample below updates the elements of an array b by performing a simple operation on the elements of an array a the parallelization is done by the openmp directive pragma omp the scheduling of tasks is dynamic notice how the iteration counters j and k have to be made private whereas the primary iteration counter i is private by default the task of running through i is divided among multiple threads and each thread creates its own versions of j and k in its execution stack thus doing the full task allocated to it and updating the allocated part of the array b at the same time as the other threads the next code sample is a common usage of the reduction clause to calculate reduced sums here we add up all the elements of an array a with an idependent weight using a for loop which we parallelize using openmp directives and reduction clause the scheduling is kept static an equivalent less elegant implementation of the above code is to create a local sum variable for each thread locsum and make a protected update of the global variable sum at the end of the process through the directive critical note that this protection is critical as explained elsewhere implementations openmp has been implemented in many commercial compilers for instance visual c     and  support it openmp  in professional team system premium and ultimate editions as well as intel parallel studio for various processors oracle solaris studio compilers and tools support the latest openmp specifications with productivity enhancements for solaris os ultrasparc and xx and linux platforms the fortran c and c compilers from the portland group also support openmp  gcc has also supported openmp since version  compilers with an implementation of openmp  several compilers support openmp  compilers supporting openmp  autoparallelizing compilers that generates source code annotated with openmp directives several profilers and debuggers expressly support openmp pros and cons pros cons performance expectations one might expect to get an n times speedup when running a program parallelized using openmp on a n processor platform however this seldom occurs for these reasons thread affinity some vendors recommend setting the processor affinity on openmp threads to associate them with particular processor cores this minimizes thread migration and contextswitching cost among cores it also improves the data locality and reduces the cachecoherency traffic among the cores or processors benchmarks there are some public domain openmp benchmarks for users to try